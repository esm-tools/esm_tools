################################################################################
# The albedo.awi.de yaml config file for esm-tools
#
# P. Gierz, Nov/Dec 2022
# M. Andres-Martinez, Nov/Dec 2022
################################################################################
name: albedo
# Could be useful if this is extraced from /etc/os-distro:
operating_system: "linux-rocky"
sh_interpreter: "/usr/bin/bash"
# Batch system and configuration:
batch_system: "slurm"
accounting: true
further_reading:
    - batch_system/slurm.yaml
partitions:
    compute:
        name: mpp
        cores_per_node: 36
    pp:
        name: smp
        cores_per_node: 1

# compute node hardware information:
logical_cpus_per_core: 2
threads_per_core: 1

# pool directory information:
pool_directories:
    pool: "/albedo/pool"
    projects: "/albedo/work/projects"
    focipool: "/dev/null"
pool_dir: "${computer.pool_directories.pool}"

hyper_flag: "" # No hyperthreading on albedo is supported in esm-tools (yet)!
choose_heterogeneous_parallelization:
    True:
        choose_taskset:
            False:
                add_export_vars:
                    I_MPI_SLURM_EXT: 0
            "*":
                taskset_chs: ""
        add_unset_vars:
            - "SLURM_DISTRIBUTION"
            - "SLURM_NTASKS"
            - "SLURM_NPROCS"
            - "SLURM_ARBITRARY_NODELIST"

hetjob_flag: packjob

unset_vars:
    - "SLURM_MEM_PER_NODE"
    - "SLURM_MEM_PER_CPU"

mpi_implementation: openmpi
choose_mpi_implementation:
    intel-oneapi-mpi:
        mpi_implementation_module: intel-oneapi-mpi/2021.6.0
        mpi_implementation_suffix: intel-oneapi-mpi2021.6.0
    openmpi:
        mpi_implementation_module: openmpi/4.1.3
        mpi_implementation_suffix: openmpi4.1.3

compiler_suite: intel-oneapi
choose_compiler_suite:
    gcc:
        compiler_module: gcc/12.1.0
        compiler_suffix: gcc12.1.0
        fc: mpifort
        f77: mpifort
        mpifc: mpifort
        mpicc: mpicc
        cc: mpicc
        cxx: mpicxx
    intel-oneapi:
        compiler_module: intel-oneapi-compilers/2022.1.0 intel-oneapi-mkl/2022.1.0
        compiler_suffix: oneapi2022.1.0
        fc: mpif90
        f77: mpif90
        mpifc: mpif90
        mpicc: mpicc
        cc: mpicc
        cxx: mpic++

netcdf_c_version: "4.8.1"
netcdf_cxx_version: "4.2"
netcdf_fortran_version: "4.5.4"
hdf5_version: "1.12.2"

module_actions:
    # Ensure a clean environment before start:
    - "purge"
    # Compilers:
    #- "load ${compiler_module}"
    # MPI Implementation:
    #- "load ${mpi_implementation_module}-${compiler_suffix}"
    # Libraries:
    # FIXME(PG): @sebhinck do we need to get a specific udunits version here as well, maybe?
    - "load udunits/2.2.28"
    #- "load netcdf-c/${netcdf_c_version}-${mpi_implementation_suffix}-${compiler_suffix}"
    #- "load netcdf-cxx/${netcdf_cxx_version}-${mpi_implementation_suffix}-${compiler_suffix}"
    #- "load netcdf-fortran/${netcdf_fortran_version}-${mpi_implementation_suffix}-${compiler_suffix}"
    #- "load hdf5/${hdf5_version}-${mpi_implementation_suffix}-${compiler_suffix}"
    - load /albedo/home/mandresm/.spack/modules/intel-oneapi-compilers-2022.0.1-gcc-12.1.0-mech5is
    - load /albedo/home/mandresm/.spack/modules/intel-oneapi-mkl-2022.0.1-gcc-12.1.0-ebd776q
    - load /albedo/home/mandresm/.spack/modules/openmpi-4.1.3-intel-2021.5.0-uyqbpcs
    - load /albedo/home/mandresm/.spack/modules/hdf5-1.12.2-intel-2021.5.0-okkbjga
    - load /albedo/home/mandresm/.spack/modules/netcdf-c-4.8.1-intel-2021.5.0-24hqns7
    - load /albedo/home/mandresm/.spack/modules/netcdf-fortran-4.5.4-intel-2021.5.0-h37jgao
    - load /albedo/home/mandresm/.spack/modules/eccodes-2.25.0-intel-2021.5.0-bjv3v4v
    # General Tools:
    - "load cdo/2.0.5"
    - "load nco/5.0.1"
    - "load git/2.35.2"
    #- "load perl/5.35.0-gcc12.1.0"
    - "load python/3.10.4"
    # Show what is there in the log:
    - "list"

export_vars:
    # Basics:
    LC_ALL: en_US.UTF-8

    # Compilers:
    FC: ${fc}
    F77: ${f77}
    MPIFC: ${mpifc}
    MPICC: ${mpicc}
    CC: ${cc}
    CXX: ${cxx}

    # Message Passing Interface (MPI) environment variables:
    MPIROOT: "$(${mpifc} -show | perl -lne 'm{ -I(.*?)/include } and print $1')"
    MPI_LIB: "$(${mpifc} -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')"

    # HDF5 exported environment variables:
    #HDF5_C_I/usr/lib64/perl5HDF5ROOT/include

    #PSMPIFLAGS: '"-lrt -lm -ldl"'

    # Linear-Algebra Library environment variables:
    LAPACK_LIB: "'-mkl=sequential'"
    #LAPACK_LIB: '"-lmkl_intel_lp64 -lmkl_core -lpthread  -ldl"'

    OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'

    # PERL library
    #PERL5LIB: "/albedo/soft/sw/spack-sw/perl/5.35.0-asf6m5t/lib/5.35.0/"
    PERL5LIB: "/albedo/home/mandresm/my_libs/perl-5.32.0/lib"

    # AWICM3
    #SZIPROOT: "/sw/spack-levante/libaec-1.0.5-gij7yv"
    HDF5ROOT: "/albedo/home/mandresm/.spack/sw/hdf5/1.12.2-okkbjga/"
    NETCDFROOT: "/albedo/home/mandresm/.spack/sw/netcdf-c/4.8.1-24hqns7/"
    NETCDFFROOT: "/albedo/home/mandresm/.spack/sw/netcdf-fortran/4.5.4-h37jgao/"
    ECCODESROOT: "/albedo/home/mandresm/.spack/sw/eccodes/2.25.0-hwsa4h3/"

    PATH: "$PERL5LIB/../bin:$PATH" #$HDF5ROOT/bin:$NETCDFFROOT/bin:$NETCDFROOT/bin:$ECCODESROOT/bin:$PATH

    LD_LIBRARY_PATH[(1)]: /albedo/home/mandresm/.spack/sw/intel-oneapi-mkl/2022.0.1-ebd776q/mkl/2022.0.1/lib/intel64:$LD_LIBRARY_PATH
