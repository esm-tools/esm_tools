# LEVANTE YAML CONFIGURATION FILES

name: levante
#hyper_flag: "--cpus-per-task=1"
#additional_flags: ["--mem=0"] # TODO@Sebastian: why was this line commented out?
account: None

use_hyperthreading: False
choose_use_hyperthreading:
        "1":
                hyperthreading_flag: ""
                mt_launcher_flag: ""
        True:
                hyperthreading_flag: ""
                mt_launcher_flag: ""
        "0":
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                                mt_launcher_flag: "--hint=nomultithread "
                        True:
                                hyperthreading_flag: ""
                                mt_launcher_flag: "--hint=nomultithread "
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
        False:
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                                mt_launcher_flag: "--hint=nomultithread "
                        True:
                                hyperthreading_flag: ""
                                mt_launcher_flag: "--hint=nomultithread "
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"

accounting: true

batch_system: "slurm"
hetjob_flag: hetjob
jobtype: compute
sh_interpreter: "/bin/bash"
partition: compute

choose_partition:
        compute:
                partition_name: "compute"
                partition_cpn: 128

partitions:
        compute:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}
        pp:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}

logical_cpus_per_core: 2
threads_per_core: 1

pool_directories:
        pool: "/pool/data"
        focipool: "/work/bb0519/foci_input2"

pool_dir:  "/pool/data"

# kh 15.02.22 preferred on levante (at the moment), on mistral intelmpi was the default
compiler_mpi: intel2022_openmpi

# seb-wahl: choose libaries below, default is the system libs via modules
iolibraries: system_libs

module_actions:
        - "purge"
        - "load git/2.31.1-gcc-11.2.0"
        - "load cdo/2.0.5-gcc-11.2.0"
        - "load nco/5.0.6-gcc-11.2.0"

# kh 31.05.22 enable for ddt
#       - "load arm-forge/21.1.3-gcc-11.2.0"



export_vars:
        # Locale Settings
        LC_ALL: en_US.UTF-8

        # kh 21.02.22 support for cpu model dependent compiler options
        CPU_MODEL: AMD_EPYC_ZEN3
        FESOM_PLATFORM_STRATEGY: levante.dkrz.de

# kh 07.03.22
        I_MPI_PMI_LIBRARY: /usr/lib64/libpmi2.so
        I_MPI_PMI: pmi2
        FI_PROVIDER: mlx
        I_MPI_OFI_PROVIDER: mlx
        I_MPI_FABRICS: shm:ofi

#       I_MPI_HYDRA_BOOTSTRAP: slurm
#       I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS: "--ntasks-per-node=1"

# kh 18.02.22 also see https://gitlab.dkrz.de/levante/early-access/-/wikis/home
        HCOLL_ENABLE_MCAST_ALL: "1"
        HCOLL_MAIN_IB: mlx5_0:1
        UCX_IB_ADDR_TYPE: ib_global
        UCX_NET_DEVICES: mlx5_0:1
        UCX_TLS: mm,knem,cma,dc_mlx5,dc_x,self
        UCX_UNIFIED_MODE: y

# kh 16.06.22 set appropriate pinning for OpenMP
        KMP_LIBRARY: turnaround
        KMP_AFFINITY: "granularity=fine,scatter"


# Variables needed exclusively during runtime
# NOTE: runtime_environment_changes does not work from the machine files
choose_general.run_or_compile:
    runtime:
        add_export_vars:
            OMPI_MCA_btl: self
            OMPI_MCA_coll: "^ml"
            OMPI_MCA_coll_hcoll_enable: "1"
            OMPI_MCA_io: "romio321"
            OMPI_MCA_osc: "pt2pt"
            OMPI_MCA_pml: "ucx"

choose_compiler_mpi:
       gnu11_ompi2022:
                # ESM-Tools vars necessary for ``system_libs`` case
                #netcdf_cxx_root: /sw/spack-levante/netcdf-cxx-4.2-q3vhdk #JS: copied from intel2022_impi2021; does this do anything?
                add_module_actions:
                        - "load intel-oneapi-compilers/2022.0.1-gcc-11.2.0"
                        - "load intel-oneapi-mkl/2022.0.1-gcc-11.2.0"
                        - "load openmpi/4.1.2-gcc-11.2.0"                       

                add_export_vars:
                        FC: mpif90
                        F77: mpi77
                        MPICC: mpicc
                        MPIFC: mpif90
                        CC: mpicc
                        CXX: mpic++
                
       intel2022_impi2021:
                # ESM-Tools vars necessary for ``system_libs`` case
                mod_netcdf_c: netcdf-c/4.8.1-intel-oneapi-mpi-2021.5.0-intel-2021.5.0
                mod_netcdf_f: netcdf-fortran/4.5.3-intel-oneapi-mpi-2021.5.0-intel-2021.5.0
                mod_hdf5: hdf5/1.12.1-intel-oneapi-mpi-2021.5.0-intel-2021.5.0
                netcdf_cxx_root: /sw/spack-levante/netcdf-cxx-4.2-q3vhdk
                add_module_actions:
                        - "load intel-oneapi-compilers/2022.0.1-gcc-11.2.0"
                        - "load intel-oneapi-mkl/2022.0.1-gcc-11.2.0"
                        - "load intel-oneapi-mpi/2021.5.0-intel-2021.5.0"
                add_export_vars:
                        FC: mpiifort
                        F77: mpiifort
                        MPICC: mpiicc
                        MPIFC: mpiifort
                        CC: mpiicc
                        CXX: mpiicpc
# kh 15.02.22 todo: check
                        MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
                        MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

       intel2022_openmpi:
                # ESM-Tools vars necessary for ``system_libs`` case
                mod_netcdf_c: netcdf-c/4.8.1-openmpi-4.1.2-intel-2021.5.0
                mod_netcdf_f: netcdf-fortran/4.5.3-openmpi-4.1.2-intel-2021.5.0
                mod_hdf5: hdf5/1.12.1-openmpi-4.1.2-intel-2021.5.0
                netcdf_cxx_root: /work/ab0995/HPC_libraries/intel-oneapi-compilers/2022.0.1-gcc-11.2.0/openmpi/4.1.2-intel-2021.5.0 #/sw/spack-levante/netcdf-cxx-4.2-selbph
                add_module_actions:
                        - "load intel-oneapi-compilers/2022.0.1-gcc-11.2.0"
                        - "load intel-oneapi-mkl/2022.0.1-gcc-11.2.0"
                        - "load openmpi/4.1.2-intel-2021.5.0"

                add_export_vars:
                        FC: mpif90
                        F77: mpif90
                        MPICC: mpicc
                        MPIFC: mpif90
                        CC: mpicc
                        CXX: mpicxx
# kh 15.02.22 todo: check
                        MPIROOT: "\"$(mpif90 -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
                        MPI_LIB: "\"$(mpif90 -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""


choose_iolibraries:
    system_gnu_libs: #Copy paste from DE IFS env.sh
        add_module_actions:
            - "unload cdo"
            - "load gcc/11.2.0-gcc-11.2.0"
            - "load fftw/3.3.10-gcc-11.2.0"
            - "load hdf5/1.12.1-gcc-11.2.0"
            - "load netcdf-c/4.8.1-gcc-11.2.0"
            - "load netcdf-fortran/4.5.3-gcc-11.2.0"
            - "load git/2.31.1-gcc-11.2.0"
            - "load libaec/1.0.5-gcc-11.2.0"
        add_export_vars:
            LD_RUN_PATH: $LD_LIBRARY_PATH
            PATH: /work/ab0246/HPC_libraries/intel-oneapi-compilers/2022.0.1-gcc-11.2.0/openmpi/4.1.2-gcc-11.2.0/bin:/sw/spack-levante/cmake-3.22.1-5bz6zc/bin:$PATH
            AEC_ROOT: /sw/spack-levante/libaec-1.0.5-gij7yv
            aec_ROOT: /sw/spack-levante/libaec-1.0.5-gij7yv
            NetCDF_C_ROOT: /sw/spack-levante/netcdf-c-4.8.1-qk24yp
            NetCDF_Fortran_ROOT: /sw/spack-levante/netcdf-fortran-4.5.3-l2ulgp
            NETCDF_PATH: $NetCDF_C_ROOT
            NETCDF_ROOT: $NetCDF_Fortran_ROOT
            NETCDFROOT: $NetCDF_Fortran_ROOT
            TBBROOT: /sw/spack-levante/intel-oneapi-compilers-2022.0.1-an2cbq/tbb/2021.5.0
            TBBMALLOC_DIR: $TBBROOT/lib/intel64/gcc4.8
            MPI_HOME: /sw/spack-levante/openmpi-4.1.2-yfwe6t
            MPI_ROOT: /sw/spack-levante/openmpi-4.1.2-yfwe6t
            mpi_ROOT: /sw/spack-levante/openmpi-4.1.2-yfwe6t
            MPI_DIR: /sw/spack-levante/openmpi-4.1.2-yfwe6t
            mpi_DIR: /sw/spack-levante/openmpi-4.1.2-yfwe6t
            LD_LIBRARY_PATH: $MPI_HOME/lib:$TBBMALLOC_DIR:$LD_LIBRARY_PATH

    system_libs:
        add_module_actions:
            - "load ${mod_netcdf_c}"
            - "load ${mod_netcdf_f}"
            - "load ${mod_hdf5}"

        add_export_vars:
            IO_LIB_ROOT: "/work/ab0246/HPC_libraries/intel-oneapi-compilers/2022.0.1-gcc-11.2.0/openmpi/4.1.2-intel-2021.5.0"
            # NOTE: Could be set by the module
            HDF5ROOT: "/sw/spack-levante/hdf5-1.12.1-tvymb5"
            HDF5_C_INCLUDE_DIRECTORIES: $HDF5ROOT/include
            HDF5_ROOT: $HDF5ROOT
            NETCDFFROOT: /sw/spack-levante/netcdf-fortran-4.5.3-k6xq5g
            NETCDFROOT: /sw/spack-levante/netcdf-c-4.8.1-2k3cmu
            NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
            NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
            NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
            NETCDF_CXX_LIBRARIES: $NETCDFROOT/lib
            # flags required for ECHAM6 (and possibly other models)
            # compilation with cmake if parastationMPI or openMPI is used
            # contact seb-wahl for details
            PSMPIFLAGS: '"-lrt -lm -ldl"'

            # Linear Algebra (LAPACK)
            LAPACK_LIB: "'-mkl=sequential'"

            # SZIPROOT: TODO
            ZLIBROOT: /usr

            # seb-wahl: OASIS3MCT_FC_LIB is used by ECHAM, since it's unclear whether
            # it's used by other models it stays in levante.yaml for now
            OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'

            # kh 17.02.22 orig mistral, not yet tested on levante (but should work)
            PERL5LIB: /usr/lib64/perl5

            LD_LIBRARY_PATH[(0)]: $HDF5ROOT/lib:$NETCDFROOT/lib:$NETCDFFROOT/lib:$LD_LIBRARY_PATH

            # avoid GLIBCXX_3.4.15 not found error
            # MA: This solves nothing for AWICM3
            #LD_LIBRARY_PATH[(1)]: /sw/spack-levante/gcc-11.2.0-bcn7mb/lib64:$LD_LIBRARY_PATH

            # kh 01.03.22 current philosophy on levante regarding the modules
            # concept is to avoid LD_LIBRARY_PATH as far as possible to make
            # everything clearer

            # kh 17.02.22 todo (not yet required for component fesom-2.1-recom-par_tracers)
            #       LD_LIBRARY_PATH[(1)]: ...

            # The following line is also moved to echam.yaml, only active for mistral and paleodyn echam6:
            # - "LD_LIBRARY_PATH=/sw/rhel6-x64/netcdf/parallel_netcdf-1.6.1-impi-intel14/lib/:$LD_LIBRARY_PATH"

            # kh 17.02.22 currently required to use mkl
            LD_LIBRARY_PATH[(1)]: /sw/spack-levante/intel-oneapi-mkl-2022.0.1-ttdktf/mkl/2022.0.1/lib/intel64:$LD_LIBRARY_PATH

            # kh 07.03.22 currently required to use Intel MPI
            LD_LIBRARY_PATH[(2)]: /sw/spack-levante/intel-oneapi-mpi-2021.5.0-mrcss7/mpi/2021.5.0/libfabric/lib:$LD_LIBRARY_PATH

    # Work done on self-compiled libraries by Jan Streffing, currently in progress and untested.
    awi_paleodyn_libs:
        choose_compiler_mpi:
            intel2022_openmpi:
                add_export_vars:
                    IO_LIB_ROOT: /work/ab0246/HPC_libraries/intel-oneapi-compilers/2022.0.1-gcc-11.2.0/openmpi/4.1.2-intel-2021.5.0/
                    PATH: $IO_LIB_ROOT/bin:$PATH
                    LD_LIBRARY_PATH: $IO_LIB_ROOT/lib:$LD_LIBRARY_PATH:/sw/spack-levante/openblas-0.3.18-mzclcq/include:/sw/spack-levante/nvhpc-21.11-5dvdc4/Linux_x86_64/21.11/compilers/include

                    SZIPROOT: $IO_LIB_ROOT
                    HDF5ROOT: $IO_LIB_ROOT
                    HDF5_ROOT: $HDF5ROOT
                    NETCDFROOT: $IO_LIB_ROOT
                    NETCDFFROOT: $IO_LIB_ROOT
                    ECCODESROOT: $IO_LIB_ROOT

                    HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
                    NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
                    NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
                    NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
                    OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'           
                    # kh 07.03.22 currently required to use Intel MPI
                    LD_LIBRARY_PATH[(2)]: '$I_MPI_ROOT/libfabric/lib:$LD_LIBRARY_PATH'

    geomar_libs:
       add_export_vars:
          IO_LIB_ROOT: "<WILL_BE_OVERWERITTEN>"
          PATH: $IO_LIB_ROOT/bin:$PATH
          LD_LIBRARY_PATH: $IO_LIB_ROOT/lib:$LD_LIBRARY_PATH

          SZIPROOT: $IO_LIB_ROOT
          HDF5ROOT: $IO_LIB_ROOT
          HDF5_ROOT: $HDF5ROOT
          NETCDFROOT: $IO_LIB_ROOT
          NETCDFFROOT: $IO_LIB_ROOT
          ECCODESROOT: $IO_LIB_ROOT

          HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
          NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
          NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'
       choose_compiler_mpi:
            intel2022_impi2021:
                add_export_vars:
                    IO_LIB_ROOT: /work/bb0519/HPC_libraries/intel2022.0.1_impi2021.5.0_20220318/ 
                    # kh 07.03.22 currently required to use Intel MPI
                    LD_LIBRARY_PATH[(2)]: '$I_MPI_ROOT/libfabric/lib:$LD_LIBRARY_PATH'
            intel2022_openmpi:
                    IO_LIB_ROOT: /work/bb0519/HPC_libraries/intel2022.0.1_ompi4.1.2_20220420 

fc: "$FC"
cc: "$CC"
mpifc: "$MPIFC"
mpicc: "$MPICC"
cxx: "$CXX"

# kh 23.06.22 do not use --distribution=plane=128 for hybrid (MPI + OpenMP) jobs
# launcher_flags: "-l --distribution=plane=128 --cpu-bind=${cpu_bind}"

# kh 23.06.22 should be set generally in accordance with 
# the tips from DKRZ-TechTalk "From Mistral to Levante" June 22, 2022
launcher_flags: "-l" # --distribution=block:cyclic"

further_reading:
        - batch_system/slurm.yaml
