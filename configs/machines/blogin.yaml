# BLOGIN YAML CONFIGURATION FILES

name: blogin
account: None

# set default for hyperthreading_flag
use_hyperthreading: False
# seb-wahl: use old heterogeneous parallelization on HLRN4, the new approach does not work yet
taskset: true
choose_use_hyperthreading:
        "1":
                hyperthreading_flag: ""
        True:
                hyperthreading_flag: ""
        "0":
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmix -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
        False:
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmix -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"

accounting: true
batch_system: "slurm"
jobtype: compute
sh_interpreter: "/bin/bash"
partition: cpu-clx:test 

choose_partition:
        standard96:
                partition_name: standard96
                partition_cpn: 96
        'standard96:test':
                partition_name: 'standard96:test'
                partition_cpn: 96
        'cpu-clx:test':
                partition_name: 'cpu-clx:test'
                partition_cpn: 96

partitions:
        compute:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}
        pp:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}


logical_cpus_per_core: 2
threads_per_core: 1
hetjob_flag: packjob

pool_directories:
        pool: "/scratch/usr/hbkawi"
        focipool: "/scratch/usr/shkifmsw/foci_input2"

pool_dir:  "/scratch/usr/hbkawi"

# default settings for compiler, mpi and I/O libs
# TODO: system_libs not yet properly configured as I (seb-wahl) don't use them
compiler_mpi: intel2024_impi2021
iolibraries: geomar_libs

# basic modules and export vars needed
# for all compiler and I/O settings

export_vars:
   LC_ALL: en_US.UTF-8
   # Recommended by HLNR support when using an MPI binary and srun
   # removed by seb-wahl as it slows down ECHAM6 by 50% 
   #SLURM_CPU_BIND: none

choose_compiler_mpi:
   intel2024_impi2021:
      add_module_actions:
         - "load intel/2024.2"
         - "load impi/2021.13"
         # TODO: check whether loading gcc is still required
         #- "load gcc/13.3.0"
      add_export_vars:
         FC: mpiifx
         F77: mpiifx
         MPIFC: mpiifx
         CC: mpiicx
         CXX: mpiicpx
         FCFLAGS: -free
         MPIROOT: "\"$(mpiifx -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifx -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

choose_iolibraries:

    # not yet configured for berlin as of August 2024 as e.g. netcdf is not yet available
    # as a module
    system_libs:
       # TODO: find the correct libraries and dependencies
       add_module_actions:
         - "load hdf5-parallel/impi/intel/1.14.4" 
       # TODO: find the correct libraries and dependencies
       add_export_vars:

    geomar_libs:
       add_export_vars:
          IO_LIB_ROOT: "<WILL_BE_OVERWERITTEN>"
          PATH: $IO_LIB_ROOT/bin:$PATH

          SZIPROOT: $IO_LIB_ROOT
          HDF5ROOT: $IO_LIB_ROOT
          HDF5_ROOT: $HDF5ROOT
          NETCDFROOT: $IO_LIB_ROOT
          NETCDFFROOT: $IO_LIB_ROOT
          ECCODESROOT: $IO_LIB_ROOT

          HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
          NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
          NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'
          
          # For OASIS3-MCT5 from CERFACS 
          OASIS_NETCDF: $NETCDFROOT
          OASIS_NETCDFF: $NETCDFFROOT

          LD_LIBRARY_PATH: $IO_LIB_ROOT/lib:$LD_LIBRARY_PATH

       choose_compiler_mpi:

          intel2024_impi2021:
             add_export_vars:
                IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2024.2_impi2021.13_20240723
                HDF5ROOT: $IO_LIB_ROOT/HDF_Group/HDF5/1.14.4.3
                LD_LIBRARY_PATH: $IO_LIB_ROOT/lib:$HDF5ROOT/lib:$LD_LIBRARY_PATH

# some yamls use computer.fc, etc to identify the compiler, so we need to add them
fc: "$FC"
cc: "$CC"
mpifc: "$MPIFC"
mpicc: "$MPICC"
cxx: "$CXX"

# TODO: or stay with pmi2 ?
launcher_flags: "--mpi=pmix -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind} --distribution=cyclic:cyclic --export=ALL"

further_reading:
        - batch_system/slurm.yaml
