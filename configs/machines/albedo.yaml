################################################################################
# The albedo.awi.de yaml config file for esm-tools
#
# P. Gierz, Nov/Dec 2022
# M. Andres-Martinez, Nov/Dec 2022
################################################################################
name: albedo
# Could be useful if this is extraced from /etc/os-distro:
operating_system: "linux-rocky"
sh_interpreter: "/usr/bin/bash"
# Batch system and configuration:
batch_system: "slurm"
accounting: true
further_reading:
  - batch_system/slurm.yaml
partitions:
  compute:
    name: mpp
    cores_per_node: 36
  pp:
    name: smp
    cores_per_node: 1

# compute node hardware information:
logical_cpus_per_core: 2
threads_per_core: 1

# pool directory information:
pool_directories:
  pool: "/albedo/pool"
  projects: "/albedo/work/projects"
  focipool: "/dev/null"
pool_dir: "${computer.pool_directories.pool}"

hyper_flag: "" # No hyperthreading on albedo is supported in esm-tools (yet)!
choose_heterogeneous_parallelization:
  True:
    choose_taskset:
      False:
          add_export_vars:
              I_MPI_SLURM_EXT: 0
      "*":
          taskset_chs: ""
    add_unset_vars:
      - "SLURM_DISTRIBUTION"
      - "SLURM_NTASKS"
      - "SLURM_NPROCS"
      - "SLURM_ARBITRARY_NODELIST"

hetjob_flag: packjob

unset_vars:
  - "SLURM_MEM_PER_NODE"
  - "SLURM_MEM_PER_CPU"

mpi_implementation: openmpi
choose_mpi_implementation:
  intel-oneapi-mpi:
    mpi_implementation_module: intel-oneapi-mpi/2021.6.0
    mpi_implementation_suffix: intel-oneapi-mpi2021.6.0
  openmpi:
    mpi_implementation_module: openmpi/4.1.3
    mpi_implementation_suffix: openmpi4.1.3

compiler_suite: gcc
choose_compiler_suite:
  gcc:
    compiler_module: gcc/12.1.0
    compiler_suffix: gcc12.1.0
    # FIXME(SOMEONE PLEASE): The module you load determines which compilers you 
    # want to use. This sort of information should (in my view) *NOT* be hard-coded
    # into our generated compile script.
    fc: "gfortran"
    cc: "gcc"
    add_export_vars:
      # NOTE(PG):
      #
      # I am not sure if the intel compilers need the same flags.
      #
      # The gfortran used on albedo is rather strict and we have many models 
      # with legacy code which does not follow the newest standards, thus we 
      # need to turn a few errors into just warnings in the compiler. It would 
      # be better to fix the code of the individual models so this is not 
      # needed:
      FFLAGS: "'-fallow-argument-mismatch'"
      FCLAGS: "'-fallow-argument-mismatch'"
  intel-oneapi:
    compiler_module: intel-oneapi-compilers/2022.1.0
    compiler_suffix: oneapi2022.1.0

netcdf_c_version: "4.8.1"
netcdf_cxx4_version: "4.3.1"
netcdf_fortran_version: "4.5.4"
hdf5_version: "1.12.2"

# module_actions:
#         # Ensure a clean environment before start:
#         - "purge"
#         # Compilers:
#         - "load /albedo/home/pgierz/spack/share/spack/modules/linux-rocky8-zen/intel-oneapi-compilers-2022.0.1-gcc-8.5.0-scqri3c"
#         # MPI Implementation:
#         - "load /albedo/home/pgierz/spack/share/spack/modules/linux-rocky8-zen2/openmpi-4.1.4-intel-2021.5.0-mkiypx5"
#         # Libraries:
#         # FIXME(PG): @sebhinck do we need to get a specific udunits version here as well, maybe?
#         - "load udunits/2.2.28"
#         # NOTE(PG): The module versions **with** mpi in the module name are for parallel NetCDF, 
#         # not needed in many cases
#         # - "load netcdf-c/${netcdf_c_version}-${compiler_suffix}"
#         - "load /albedo/home/pgierz/spack/share/spack/modules/linux-rocky8-zen2/netcdf-c-4.8.1-intel-2021.5.0-ilrhnqu"
#         # - "load netcdf-cxx4/${netcdf_cxx4_version}-${compiler_suffix}"
#         - "load /albedo/home/pgierz/spack/share/spack/modules/linux-rocky8-zen2/netcdf-cxx4-4.3.1-intel-2021.5.0-vqwu43z"
#         # - "load netcdf-fortran/${netcdf_fortran_version}-${compiler_suffix}"
#         - "load /albedo/home/pgierz/spack/share/spack/modules/linux-rocky8-zen2/netcdf-fortran-4.6.0-intel-2021.5.0-hcqpk2g"
#         # - "load hdf5/${hdf5_version}-${compiler_suffix}"
#         - "load /albedo/home/pgierz/spack/share/spack/modules/linux-rocky8-zen2/hdf5-1.12.2-intel-2021.5.0-tenv4fn"
#         # Math Kernel Library:
#         - "load /albedo/home/pgierz/spack/share/spack/modules/linux-rocky8-zen2/intel-oneapi-mkl-2022.1.0-intel-2021.5.0-dzpadhb"
#         # General Tools:
#         - "load cdo/2.0.5"
#         - "load nco/5.0.1"
#         - "load git/2.35.2"
#         - "load python/3.10.4"
#         # Show what is there in the log:
#         - "list"

spack_actions:
  - "activate esm-tools-albedo"


export_vars:
  # Basics:
  LC_ALL: en_US.UTF-8

  # NOTE(PG): All Compilers are set via the module files!
  # Compilers:
  # FC: ${fc}
  # F77: ${f77}
  # MPIFC: ${mpifc}
  # MPICC: ${mpicc}
  # CC: ${cc}
  # CXX: ${cxx}

  # Message Passing Interface (MPI) environment variables:
  # MPIROOT: "$(${mpifc} -show | perl -lne 'm{ -I(.*?)/include } and print $1')"
  # NOTE(PG): I do not like this....should be set via module file, or fixed in model
  MPI_LIB: "$($MPIF90 -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')"

  # Linear-Algebra Library environment variables:
  # FIXME(PG): This would be nicer...
  # LAPACK_LIB: "$(mkl_link_tool --quiet -libs $MPIF90)"
  # NOTE(PG): I do not like this....should be set via module file, or fixed in model
  LAPACK_LIB: "'-L/albedo/soft/sw/spack-sw/intel-oneapi-mkl/2022.1.0-7235vfh/mkl/2022.1.0 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lm -ldl'" 
