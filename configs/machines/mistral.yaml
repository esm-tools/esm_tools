# MISTRAL YAML CONFIGURATION FILES

name: mistral
#hyper_flag: "--cpus-per-task=1"
additional_flags: "--mem=0"
account: None

choose_general.use_hyperthreading:
        "0":
                hyperthreading_flag: "--ntasks-per-core=1"
        "*":
                hyperthreading_flag: ""

accounting: true

"batch_system": "slurm"

jobtype: compute
sh_interpreter: "/bin/bash"

choose_jobtype:
        tidy_and_resubmit:
                partition: compute
        post:
                partition: compute
        compute: 
                partition: compute


choose_partition: 
        "compute":
                cores_per_node: 24

logical_cpus_per_core: 2

threads_per_core: 1

pool_directories:
        pool: "/pool/data"
        focipool: "/work/bb0519/foci_input2"

pool_dir:  "/pool/data"


#nemoarchfile: BULL_MISTRAL_INTEL_uncoupled_esmtools
nemoarchfile: BULL_MISTRAL_INTEL
#xiosarchfile: BULL_MISTRAL_intel_NetcdfParallel_uncoupled
xiosarchfile: BULL_MISTRAL_intel_NetcdfParallel
# standard environment setup
#
#

useMPI: intelmpi
module_actions:
        - "purge"
        - "load gcc/4.8.2"
        - "unload intel"
        - "load intel"
        - "load cdo nco"
        - "unload netcdf"
        - "load netcdf_c/4.3.2-gcc48"
        - "load python/3.5.2"
        - "load cmake/3.13.3"

export_vars:

        - 'I_MPI_FABRICS=shm:dapl'
        - 'I_MPI_FALLBACK=disable'
        - 'I_MPI_SLURM_EXT=1'
        - 'I_MPI_LARGE_SCALE_THRESHOLD=8192' 
        - 'I_MPI_DYNAMIC_CONNECTION=0'

        - 'DAPL_NETWORK_NODES=$SLURM_NNODES'
        - 'DAPL_NETWORK_PPN=$SLURM_NTASKS_PER_NODE'
        - 'DAPL_WR_MAX=500'

        - 'OMPI_MCA_pml=cm'
        - 'OMPI_MCA_mtl=mxm'
        - 'OMPI_MCA_coll=^ghc'
        - 'OMPI_MCA_mtl_mxm_np=0'

        - 'MXM_RDMA_PORTS=mlx5_0:1'
        - 'MXM_LOG_LEVEL=FATAL'

        - 'HDF5ROOT=/sw/rhel6-x64/hdf5/hdf5-1.8.14-threadsafe-gcc48'
        - 'HDF5_C_INCLUDE_DIRECTORIES=$HDF5ROOT/include'
        - 'ESM_HDF5_DIR=sw/rhel6-x64/hdf5/hdf5-1.8.16-parallel-impi-intel14/'
        # NOTE(PG): The version of HDF5ROOT **WITH** an underscore is used by PISM Cmake:
        - "HDF5_ROOT=/sw/rhel6-x64/hdf5/hdf5-1.8.14-parallel-impi-intel14/"
        - "NETCDFFROOT=/sw/rhel6-x64/netcdf/netcdf_fortran-4.4.3-intel14"
        - "NETCDFROOT=/sw/rhel6-x64/netcdf/netcdf_c-4.3.2-gcc48"
        - "GRIBAPIROOT=/sw/rhel6-x64/grib_api/grib_api-1.15.0-intel14/"
        - "NETCDF_Fortran_INCLUDE_DIRECTORIES=$NETCDFFROOT/include"
        - "NETCDF_C_INCLUDE_DIRECTORIES=$NETCDFROOT/include"
        - "NETCDF_CXX_INCLUDE_DIRECTORIES=/sw/rhel6-x64/netcdf/netcdf_cxx-4.2.1-gcc48/include"
        - "ESM_NETCDF_C_DIR=/sw/rhel6-x64/netcdf/netcdf_c-4.3.2-parallel-impi-intel14/"
        - "ESM_NETCDF_F_DIR=/sw/rhel6-x64/netcdf/netcdf_fortran-4.4.3-parallel-impi-intel14/"

        - "PERL5LIB=/usr/lib64/perl5"
        - "SZIPROOT=/sw/rhel6-x64/sys/libaec-0.3.2-gcc48"
        - "LAPACK_LIB='-mkl=sequential'"
        - "LAPACK_LIB_DEFAULT='-L/sw/rhel6-x64/intel/intel-18.0.1/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential'"
        - "OASIS3MCT_FC_LIB='-L$NETCDFFROOT/lib -lnetcdff'"

        - "FC=${fc}"
        - "F77=${f77}"
        - "MPIFC=${mpifc}"
        - "CC=${cc}"
        - "CXX=${cxx}"
        - "MPIROOT=\"$(${mpifc} -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
        - "MPI_LIB=\"$(${mpifc} -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

        - "UDUNITS2_ROOT=/sw/rhel6-x64/util/udunits-2.2.26-gcc64"
        - "FFTW_ROOT=/sw/rhel6-x64/numerics/fftw-3.3.7-openmp-gcc64"
        - "PROJ4_ROOT=/sw/rhel6-x64/graphics/proj4-4.9.3-gcc48"
        - "PETSC_DIR=/sw/rhel6-x64/numerics/PETSc-3.12.2-impi2018-intel18/"
        - "PATH=/sw/rhel6-x64/gcc/binutils-2.24-gccsys/bin:${PATH}"
        - "LD_LIBRARY_PATH=/sw/rhel6-x64/grib_api/grib_api-1.15.0-intel14/lib:$NETCDFFROOT/lib:$HDF5ROOT/lib:$NETCDFROOT/lib:$SZIPROOT/lib:$PROJ4_ROOT/lib:$LD_LIBRARY_PATH"
        - 'GRIB_SAMPLES_PATH="$GRIBAPIROOT/share/grib_api/ifs_samples/grib1_mlgrib2/"'
        - 'PATH=$PATH:/mnt/lustre01/sw/rhel6-x64/devtools/fcm-2017.10.0/bin/'
        - 'OIFS_GRIB_API_INCLUDE="-I$GRIBAPIROOT/include"'
        - 'OIFS_GRIB_API_LIB="-L$GRIBAPIROOT/lib -lgrib_api_f90 -lgrib_api"'
        # OIFS_GRIB_API_LIB is used by OpenIFS CY40, OIFS_GRIB_LIB is used by CY43
        - 'OIFS_GRIB_INCLUDE="$OIFS_GRIB_API_INCLUDE"'
        - 'OIFS_GRIB_LIB="$OIFS_GRIB_API_LIB"'
        - 'OIFS_GRIB_API_BIN="$GRIBAPIROOT/bin"'
        - 'MKLROOT=/sw/rhel6-x64/intel/intel-18.0.4/compilers_and_libraries_2018/linux/mkl/lib/intel64/'
        - 'LAPACK_LIB_DEFAULT="-L$MKLROOT -lmkl_intel_lp64 -lmkl_core -lmkl_sequential"'
        - 'OIFS_OASIS_BASE=$(pwd)/oasis'
        - 'OIFS_OASIS_INCLUDE="-I$OIFS_OASIS_BASE/build/lib/psmile -I$OIFS_OASIS_BASE/build/lib/psmile/scrip -I$OIFS_OASIS_BASE/build/lib/psmile/mct -I$OIFS_OASIS_BASE/build/lib/psmile/mct/mpeu"'
        - 'OIFS_OASIS_LIB="-L$OIFS_OASIS_BASE/build/lib/psmile -L$OIFS_OASIS_BASE/build/lib/psmile/scrip -L$OIFS_OASIS_BASE/build/lib/psmile/mct -L$OIFS_OASIS_BASE/build/lib/psmile/mct/mpeu -lpsmile -lmct -lmpeu -lscrip"'
        - 'OIFS_NETCDF_INCLUDE="-I$NETCDFROOT/include"'
        - 'OIFS_NETCDF_LIB="-L$NETCDFROOT/lib -lnetcdf"'
        - 'OIFS_NETCDFF_INCLUDE="-I$NETCDFFROOT/include"'
        - 'OIFS_NETCDFF_LIB="-L$NETCDFFROOT/lib -lnetcdff"'
        - 'OIFS_FC=${mpifc}'
        - 'OIFS_FFLAGS="-r8 -fp-model precise -align array32byte -O3 -xCORE_AVX2 -g -traceback -convert big_endian -fpe0"'
        - 'OIFS_FFIXED=""'
        - 'OIFS_FCDEFS="BLAS LITTLE LINUX INTEGER_IS_INT"'
        - 'OIFS_LFLAGS=$OIFS_MPI_LIB'
        - 'OIFS_CC=${cc}'
        - 'OIFS_CFLAGS="-fp-model precise -O3 -xCORE_AVX2 -g -traceback -qopt-report=0 -fpe0"'
        - 'OIFS_CCDEFS="LINUX LITTLE INTEGER_IS_INT _ABI64 BLAS"'       

choose_useMPI:
        intelmpi:
                add_module_actions:
                        - "unload intel"
                        - "load intel/18.0.4 intelmpi/2018.5.288"
                        - "load autoconf/2.69"
                fc: mpiifort
                f77: mpiifort
                mpifc: mpiifort
                cc: mpiicc
                cxx: mpiicpc
        openmpi:
                add_module_actions:
                        - "load openmpi/2.0.2p2_hpcx-intel14"
                add_export_vars:
                        - 'OMPI_MCA_pml=cm'         # sets the point-to-point management layer
                        - 'OMPI_MCA_mtl=mxm'        # sets the matching transport layer
                        - 'MXM_RDMA_PORTS=mlx5_0:1'
                        - 'MXM_LOG_LEVEL=ERROR'
                        - 'MXM_HANDLE_ERRORS=bt'
                        - 'UCX_HANDLE_ERRORS=bt'
                fc: mpif90
                f77: mpif90
                mpifc: mpif90
                cc: mpicc
                cxx: mpicxx
        bullxmpi:
                add_module_actions:
                        - "load mxm/3.4.3082"
                        - "load fca/2.5.2431"
                        - "bullxmpi_mlx/bullxmpi_mlx-1.2.9.2"  
                add_export_vars:
                        - 'FC=mpif90'
                        - 'F77=mpif90'
                        - 'MPIFC=mpif90'
                        - 'CC=mpicc'
                        - 'CXX=mpicxx'


further_reading:
        - batch_system/slurm.yaml
