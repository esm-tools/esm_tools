# YAML CONFIGURATION FILE FOR GLOGIN (NHR) PHASE 3

name: gloginp3
account: None

# set default for hyperthreading_flag
use_hyperthreading: False
# seb-wahl: use old heterogeneous parallelization on HLRN4, the new approach does not work yet
taskset: true
choose_use_hyperthreading:
        "1":
                hyperthreading_flag: ""
        True:
                hyperthreading_flag: ""
        "0":
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
        False:
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
                                        # After suggestion from Timon (not Pumbaa) at HLRN
                                        - "SLURM_JOB_NUM_NODES" 
                                        - "SLURM_NNODES"

accounting: true

batch_system: "slurm"

jobtype: compute
sh_interpreter: "/bin/bash"

partition: 'standard96:el8'

choose_partition:
        'standard96:el8':
                partition_name: standard96:el8
                partition_cpn: 96
        
partitions:
        compute:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}
        pp:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}


logical_cpus_per_core: 2

threads_per_core: 1
hetjob_flag: packjob

pool_directories:
        pool: "/scratch/usr/hbkawi"
        focipool: "/scratch/usr/shkifmsw/foci_input2"

pool_dir:  "/scratch/usr/hbkawi"

# default settings for compiler, mpi and I/O libs
# TODO: system_libs not yet properly configured as I (seb-wahl) don't use them

# Only Intel version available 
compiler_mpi: intel2023_impi2021

# Our usual home cooked libraries do not work yet
# We use available modules instead
iolibraries: system_libs

# basic modules and export vars needed
# for all compiler and I/O settings
module_actions:
   - "purge"

#   - "load cmake"
#   - "load cdo nco"
#   - "load git"

export_vars:
   LC_ALL: en_US.UTF-8
   # Recommended by HLNR support when using an MPI binary and srun
   # removed by seb-wahl as it slows down ECHAM6 by 50% 
   #SLURM_CPU_BIND: none

choose_compiler_mpi:

   intel2023_impi2021:
      add_module_actions:
         - "load intel-oneapi-compilers/2023.2.1"
         - "load intel-oneapi-mpi/2021.10.0"
         # required for CMake
         - "load curl/8.4.0-5rlmgmu ncurses/6.4-u72r7qn zlib-ng/2.1.4-ftbye2s"
         - "load cmake/3.27.7"
         - "load git/2.42.0"
         - "load cdo/2.2.2" 
         - "load nco/5.1.6" 
      
      # Note: Intel compilers now have new names: 
      # mpiicc (C) = mpiicx
      # mpiicpc (C++) = mpiicpx
      # mpiifort (Fortran) = mpiifx
      add_export_vars:
         FC: mpiifx
         F77: mpiifx
         MPIFC: mpiifx
         FCFLAGS: -free
         CC: mpiicx
         CXX: mpiicpx
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

# Do we use available modules (system_libs)
# or have we compiled netCDF etc ourselves? (geomar_libs)
choose_iolibraries:
    
    system_libs:
    
        choose_compiler_mpi:
        
            intel2023_impi2021:
                # Modules to load 
                add_module_actions:
                    - "load hdf5/1.14.3"
                    - "load netcdf-c/4.9.2"
                    - "load netcdf-fortran/4.6.1-mpi"
                    - "load eccodes/2.25.0"
                
                add_export_vars:
                    # Run module show on the modules to see how 
                    # each module changes or sets env variables
                    HDF5_ROOT: $HDF5_MODULE_INSTALL_PREFIX
                    NETCDF_DIR: $NETCDF_C_MODULE_INSTALL_PREFIX
                    NETCDFROOT: $NETCDF_C_MODULE_INSTALL_PREFIX
                    NETCDFFROOT: $NETCDF_FORTRAN_MODULE_INSTALL_PREFIX
                    ECCODESROOT: $ECCODES_MODULE_INSTALL_PREFIX 
                    LD_LIBRARY_PATH: $NETCDF_DIR/lib/:$LD_LIBRARY_PATH
                    NETCDF_CXX_LIBRARIES: $NETCDF_DIR/lib
                    
                    # For OASIS
                    HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
                    NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
                    NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
                    NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
                    OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'
                    
                    # For OASIS3-MCT5 from CERFACS
                    OASIS_NETCDF: $NETCDF_DIR
                    OASIS_NETCDFF: $NETCDF_DIR
         
    geomar_libs:
        add_export_vars:
            IO_LIB_ROOT: "<WILL_BE_OVERWERITTEN>"
            PATH: $IO_LIB_ROOT/bin:$PATH
            LD_LIBRARY_PATH: $IO_LIB_ROOT/lib:$LD_LIBRARY_PATH
            
            SZIPROOT: $IO_LIB_ROOT
            HDF5ROOT: $IO_LIB_ROOT
            HDF5_ROOT: $HDF5ROOT
            NETCDFROOT: $IO_LIB_ROOT
            NETCDFFROOT: $IO_LIB_ROOT
            ECCODESROOT: $IO_LIB_ROOT
 
            HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
            NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
            NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
            NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
            OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'
            
            # For OASIS3-MCT5 from CERFACS
            OASIS_NETCDF: $IO_LIB_ROOT
            OASIS_NETCDFF: $IO_LIB_ROOT 

        choose_compiler_mpi:
          
            intel2023_impi2021:
                add_export_vars:
                    IO_LIB_ROOT: /home/shkjocke/sw/HPC_libraries/intel2021.2_impi2021.2_20211007

# some yamls use computer.fc, etc to identify the compiler, so we need to add them
fc: "$FC"
cc: "$CC"
mpifc: "$MPIFC"
mpicc: "$MPICC"
cxx: "$CXX"

launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind} --distribution=cyclic:cyclic --export=ALL"

further_reading:
        - batch_system/slurm.yaml
