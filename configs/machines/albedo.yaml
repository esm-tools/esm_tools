# The albedo.awi.de YAML config file
name: albedo
# Could be usefully extraced from /etc/os-distro or something
operating_system: "linux-rocky"
# Should not be defined here:
#jobtype: compute NOTE(PG): ...why?? The computer configuration should not need to know anything about the jobtype.
sh_interpreter: "/usr/bin/bash"


# Batch system and configuration:
batch_system: "slurm"
accounting: false
add_further_reading:
  - batch_system/slurm.yaml
partitions:
        compute:
                name: mpp
                cores_per_node: 36
        pp:
                name: smp
                cores_per_node: 1

# (compute) node hardware information
logical_cpus_per_core: 2
threads_per_core: 1

pool_directories:
        pool: "/dev/null"
        projects: "/dev/null"
        focipool: "/dev/null"
# PG: Why is this...? Can't we just reference a particular pool from the list each time?
pool_dir: "${computer.pool_directories.pool}"

hyper_flag: "" # No hyperthreading on albedo
choose_heterogeneous_parallelization:
        True:
                choose_taskset:
                    False:
                        add_export_vars:
                            I_MPI_SLURM_EXT: 0
                    "*":
                        taskset_chs: ""
                add_unset_vars:
                        - "SLURM_DISTRIBUTION"
                        - "SLURM_NTASKS"
                        - "SLURM_NPROCS"
                        - "SLURM_ARBITRARY_NODELIST"

hetjob_flag: packjob

unset_vars:
        - "SLURM_MEM_PER_NODE"
        - "SLURM_MEM_PER_CPU"

useMPI: intel-oneapi-mpi

module_actions:
        # Ensure a clean environment before start:
        - "purge"
        # Globally available already:
        # Compilers:
        - "load intel-oneapi-compilers/2022.1.0"
        - "load intel-oneapi-mkl/2022.1.0"
        - "load intel-oneapi-mpi/2021.6.0"
        # Build tools:
        # General Tools:
        - "load cdo/2.0.5"
        - "load nco/5.0.1"
        - "load git/2.35.2"
        # Libraries:
        # FIXME(PG): @sebhinck needs to get a specific udunits version here as well, maybe?
        - "load udunits/2.2.28"
        - "load netcdf-c/4.8.1-openmpi4.1.3-oneapi2022.1.0"
        - "load netcdf-fortran/4.5.4-openmpi4.1.3-oneapi2022.1.0"
        - "load hdf5/1.12.2-openmpi4.1.3-oneapi2022.1.0" 
        # Use my test modules (to be removed)
        - "use /albedo/work/user/pgierz/SciComp/Projects/Model_Runtime/modulefiles"
        # Build tools:
        # - "load cmake-3.24.2-gcc-8.5.0-yaafrj6"
        - "load automake-1.16.5-gcc-8.5.0-7bskgwg"
        # Show what is there in the log:
        - "list"

export_vars:
        # Basics:
        LC_ALL: en_US.UTF-8

        # Compilers:
        FC: ${fc}
        F77: ${f77}
        MPIFC: ${mpifc}
        MPICC: ${mpicc}
        CC: ${cc}
        CXX: ${cxx}

        # Message Passing Interface (MPI) environment variables:
        MPIROOT: "$(${mpifc} -show | perl -lne 'm{ -I(.*?)/include } and print $1')"
        MPI_LIB: "$(${mpifc} -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')"

        # HDF5 exported environment variables:
        HDF5ROOT: /albedo/soft/sw/spack-sw/hdf5/1.12.2-fknnmx5/

        # NetCDF exported environment variables:
        NETCDFFROOT: /albedo/soft/sw/spack-sw/netcdf-fortran/4.5.4-lzqfsg3/
        NETCDFROOT: /albedo/soft/sw/spack-sw/netcdf-c/4.8.1-sp3ulf4/
        NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFROOT/include
        NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
        NETCDF_CXX_LIBRARIES: $NETCDFROOT/lib

        # Linear-Algebra Library environment variables:
        LAPACK_LIB: '"-lmkl_intel_lp64 -lmkl_core -mkl=sequential -lpthread -lm -ldl"'
        LAPACK_LIB_DEFAULT: '"-L/global/AWIsoft/intel/2018/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential"'

choose_useMPI:
        intel-oneapi-mpi:
                fc: '"ifort -mkl"'
                f77: '"ifort -mkl"'
                mpifc: mpiifort
                mpicc: mpiicc
                cc: mpiicc
                cxx: mpiicpc
