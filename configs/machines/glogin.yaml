# BLOGIN YAML CONFIGURATION FILES

name: glogin
account: None

# set default for hyperthreading_flag
use_hyperthreading: False
# seb-wahl: use old heterogeneous parallelization on HLRN4, the new approach does not work yet
taskset: true
choose_use_hyperthreading:
        "1":
                hyperthreading_flag: ""
        True:
                hyperthreading_flag: ""
        "0":
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
        False:
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"

accounting: true

batch_system: "slurm"

jobtype: compute
sh_interpreter: "/bin/bash"


partition: standard96

choose_partition:
        standard96:
                partition_name: standard96
                partition_cpn: 96
        'standard96:test':
                partition_name: 'standard96:test'
                partition_cpn: 96
        'standard96:eoptimized':
                partition_name: 'standard96:eoptimized'
                partition_cpn: 96

partitions:
        compute:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}
        pp:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}


logical_cpus_per_core: 2

threads_per_core: 1
hetjob_flag: packjob

pool_directories:
        pool: "/scratch/usr/hbkawi"
        focipool: "/scratch/usr/shkifmsw/foci_input2"

pool_dir:  "/scratch/usr/hbkawi"

# default settings for compiler, mpi and I/O libs
# TODO: system_libs not yet properly configured as I (seb-wahl) don't use them

# 18 July 2023: Intel license expired on HLRN
#compiler_mpi: intel2019_impi2019
# use this instead
#compiler_mpi: intel2022_impi2021

# to compile nemo standalone, comment the line above and uncomment the one below
#compiler_mpi: intel2019_impi2019_nemo4 
#iolibraries: system_libs
#
# for FOCIOIFS use
# compiler_mpi: intel2019_ompi
# for FOCI use
compiler_mpi: intel2023_impi2021
# for both FOCI and FOCIOIFS use
iolibraries: system_libs

# basic modules and export vars needed
# for all compiler and I/O settings
module_actions:
   - "purge"
     #   - "load slurm"
     #   - "load HLRNenv"
     #   - "load sw.skl"
     #- "load cmake"
     #   - "load cdo nco"
     #   - "load git"

export_vars:
   LC_ALL: en_US.UTF-8
   # Recommended by HLNR support when using an MPI binary and srun
   # removed by seb-wahl as it slows down ECHAM6 by 50% 
   #SLURM_CPU_BIND: none

choose_compiler_mpi:

   intel2019_impi2019_nemo4:
      add_module_actions:
         - "load intel/19.0.5"
         - "load impi/2019.5"
         - "source $I_MPI_ROOT/intel64/bin/mpivars.sh release_mt"
         - "load gcc/9.3.0"
      add_export_vars:
         FC: mpiifort
         F77: mpiifort
         MPIFC: mpiifort
         FCFLAGS: -free
         CC: mpiicc
         CXX: mpiicpc
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

   intel2019_impi2019:
      add_module_actions:
         - "load intel/19.0.5"
         - "load impi/2019.5"
      add_export_vars:
         FC: mpiifort
         F77: mpiifort
         MPIFC: mpiifort
         FCFLAGS: -free
         CC: mpiicc
         CXX: mpiicpc
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

   intel2019_ompi:
      add_module_actions:
         - "load intel/19.0.5"
         - "load openmpi/intel/3.1.6"
         - "load gcc/9.3.0"
      add_export_vars:
         FC: mpifort
         F77: mpifort
         MPIFC: mpifort
         FCFLAGS: -free
         CC: mpicc
         CXX: mpic++
         MPIROOT: "\"$(mpifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""
   
   intel2022_impi2021:
      add_module_actions:
         - "load intel/2022.2"
         - "load impi/2021.6"
         - "load gcc/9.3.0" 
      add_export_vars:
         FC: mpiifort
         F77: mpiifort
         MPIFC: mpiifort
         FCFLAGS: -free
         CC: mpiicc
         CXX: mpiicpc
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

   intel2023_impi2021:
      add_module_actions:
         - "load intel-oneapi-compilers/2023.2.1"
         - "load intel-oneapi-mpi/2021.10.0"
           #- "load intel/2022.2"
           #- "load impi/2021.6"
           #- "load gcc/9.3.0" 
      add_export_vars:
         FC: mpiifort
         F77: mpiifort
         MPIFC: mpiifort
         FCFLAGS: -free
         CC: mpiicc
         CXX: mpiicpc
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

choose_iolibraries:

    system_libs:
       # TODO: find the correct libraries and dependencies
       add_module_actions:
         - "load intel-oneapi-compilers/2023.2.1"
         - "load intel-oneapi-mpi/2021.10.0"
         - "load netcdf-fortran/4.6.1-mpi"
       # TODO: find the correct libraries and dependencies
       add_export_vars:
         NETCDF_DIR: "/sw/rev/24.05/cascadelake_opa_rocky8/linux-rocky8-cascadelake/gcc-11.4.0/netcdf-c-4.9.2-r45vhneis4kgtjbelerbeuywfo3iodp5"
         NETCDFF_DIR: "/sw/rev/24.05/cascadelake_opa_rocky8/linux-rocky8-cascadelake/oneapi-2023.2.1/netcdf-fortran-4.6.1-hk2dv2ct67ossrcdh6lhxy6eyq2kuzaa"
         HDF5ROOT: "/sw/rev/24.05/cascadelake_opa_rocky8/linux-rocky8-cascadelake/oneapi-2023.2.1/hdf5-1.14.3-6stsphcp6h4srcqryqp2vqt5e73fnb7v"
         LD_LIBRARY_PATH: $NETCDFF_DIR/lib:$NETCDF_DIR/lib:$HDF5ROOT/lib:$LD_LIBRARY_PATH
         NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDF_DIR/include
         NETCDF_CXX_LIBRARIES: $NETCDF_DIR/lib
         IO_LIB_ROOT: "/home/shkifmsw/sw/HPC_libraries/intel2022.2_impi2021.6_20230815"
         PATH: $IO_LIB_ROOT/bin:$PATH
         SZIPROOT: $IO_LIB_ROOT
         HDF5_ROOT: $HDF5ROOT
         NETCDFROOT: $NETCDF_DIR
         NETCDFFROOT: $NETCDFF_DIR
         ECCODESROOT: $IO_LIB_ROOT

         HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
         NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
         NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
         OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'
         OASIS_NETCDF: $NETCDF_DIR
         OASIS_NETCDFF: $NETCDFF_DIR
         MPEU_Fortran_INCLUDE_DIRECTORIES: "/home/shktkeme/esm/models/foci-agrif_mops_oasismct4/oasis/INSTALL_OASIS.ESMTOOLS/include"
         MPEU_Fortran_LIBRARIES: "/home/shktkeme/esm/models/foci-agrif_mops_oasismct4/oasis/INSTALL_OASIS.ESMTOOLS/lib"

    geomar_libs:
       add_export_vars:
          IO_LIB_ROOT: "<WILL_BE_OVERWERITTEN>"
          PATH: $IO_LIB_ROOT/bin:$PATH
          LD_LIBRARY_PATH: $IO_LIB_ROOT/lib:$LD_LIBRARY_PATH

          SZIPROOT: $IO_LIB_ROOT
          HDF5ROOT: $IO_LIB_ROOT
          HDF5_ROOT: $HDF5ROOT
          NETCDFROOT: $IO_LIB_ROOT
          NETCDFFROOT: $IO_LIB_ROOT
          ECCODESROOT: $IO_LIB_ROOT

          HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
          NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
          NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'

       choose_compiler_mpi:

          intel2019_impi2019_nemo4:
             add_export_vars:
                IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2019.0.5_impi2019.5_20200811

          intel2021_impi2021:
             add_export_vars:
                IO_LIB_ROOT: /home/shkjocke/sw/HPC_libraries/intel2021.2_impi2021.2_20211007
          
          # for now: use libraries compiled with older compilers
          intel2022_impi2021:
              add_export_vars:
                IO_LIB_ROOT:  /home/shkifmsw/sw/HPC_libraries/intel2022.2_impi2021.6_20230815 
                #IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2022.2_impi2021.6_20230719_save/ 
                
          intel2019_impi2019:
             add_export_vars:
                IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2019.0.5_impi2019.5_20200811

          intel2019_ompi:
             add_export_vars:
                IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2019.0.5_ompi3.1.6_20201117

# some yamls use computer.fc, etc to identify the compiler, so we need to add them
fc: "$FC"
cc: "$CC"
mpifc: "$MPIFC"
mpicc: "$MPICC"
cxx: "$CXX"

launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind} --distribution=cyclic:cyclic --export=ALL"

further_reading:
        - batch_system/slurm.yaml
