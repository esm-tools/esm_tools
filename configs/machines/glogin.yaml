# GLOGIN YAML CONFIGURATION FILES

name: glogin
account: None

# set default for hyperthreading_flag
use_hyperthreading: False
# seb-wahl: use old heterogeneous parallelization on HLRN4, the new approach does not work yet
taskset: true
choose_use_hyperthreading:
        "1":
                hyperthreading_flag: ""
        True:
                hyperthreading_flag: ""
        "0":
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
        False:
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind}"
                                add_export_vars:
                                        I_MPI_SLURM_EXT: 0
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
                                        # After suggestion from Timon (not Pumbaa) at HLRN
                                        - "SLURM_JOB_NUM_NODES" 
                                        - "SLURM_NNODES"

accounting: true

batch_system: "slurm"

jobtype: compute
sh_interpreter: "/bin/bash"


partition: standard96

choose_partition:
        standard96:
                partition_name: standard96
                partition_cpn: 96
        'standard96:el8':
                partition_name: 'standard96:el8'
                partition_cpn: 96
        'standard96:test':
                partition_name: 'standard96:test'
                partition_cpn: 96
        'standard96:eoptimized':
                partition_name: 'standard96:eoptimized'
                partition_cpn: 96

partitions:
        compute:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}
        pp:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}


logical_cpus_per_core: 2

threads_per_core: 1
hetjob_flag: hetjob

pool_directories:
        pool: "/scratch/usr/hbkawi"
        focipool: "/scratch/usr/shkifmsw/foci_input2"

pool_dir:  "/scratch/usr/hbkawi"

# default settings for compiler, mpi and I/O libs
# TODO: system_libs not yet properly configured as I (seb-wahl) don't use them

# Available options: 
# On standard96 and standard96:el7 nodes
# * intel2019_impi2019 - old software stack. Old but works. 
# * intel2019_impi2019_nemo4 - old software stack. Loads gcc/9.3.0 required for newer XIOS versions and NEMO v4. 
# * intel2022_impi2021 - old software stack. Newer. Problems with tracebacks. 
# On standard96:el8, new compute nodes. 
# * intel2023_impi2021 - new software stack. Must use system libraries. 

# Default.
# NOTE: Only for old nodes, glogin1-10. 
# For new nodes, glogin11-13, use intel2023_impi2021
compiler_mpi: intel2022_impi2021

# to compile nemo standalone, comment the line above and uncomment the one below
#compiler_mpi: 
#iolibraries: system_libs

export_vars:
   LC_ALL: en_US.UTF-8
   # Recommended by HLNR support when using an MPI binary and srun
   # removed by seb-wahl as it slows down ECHAM6 by 50% 
   #SLURM_CPU_BIND: none

module_actions:
    - "purge"

choose_compiler_mpi:

   intel2019_impi2019_nemo4:
      
      # Use GEOMAR libraries, not system modules
      iolibraries: geomar_libs
         
      add_module_actions:
         - "load slurm"
         - "load HLRNenv"
         - "load sw.skl"
         - "load cmake"
         - "load cdo nco"
         - "load git"
         - "load intel/19.0.5"
         - "load impi/2019.5"
         - "source $I_MPI_ROOT/intel64/bin/mpivars.sh release_mt"
         - "load gcc/9.3.0"
      add_export_vars:
         FC: mpiifort
         F77: mpiifort
         MPIFC: mpiifort
         FCFLAGS: -free
         CC: mpiicc
         CXX: mpiicpc
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

   intel2019_impi2019:
      
      # Use GEOMAR libraries, not system modules
      iolibraries: geomar_libs
      
      add_module_actions:
         - "load slurm"
         - "load HLRNenv"
         - "load sw.skl"
         - "load cmake"
         - "load cdo nco"
         - "load git"
         - "load intel/19.0.5"
         - "load impi/2019.5"
      add_export_vars:
         FC: mpiifort
         F77: mpiifort
         MPIFC: mpiifort
         FCFLAGS: -free
         CC: mpiicc
         CXX: mpiicpc
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

   intel2019_ompi:
      
      # Use GEOMAR libraries, not system modules
      iolibraries: geomar_libs
      
      add_module_actions:
         - "load slurm"
         - "load HLRNenv"
         - "load sw.skl"
         - "load cmake"
         - "load cdo nco"
         - "load git"
         - "load intel/19.0.5"
         - "load openmpi/intel/3.1.6"
         - "load gcc/9.3.0"
      add_export_vars:
         FC: mpifort
         F77: mpifort
         MPIFC: mpifort
         FCFLAGS: -free
         CC: mpicc
         CXX: mpic++
         MPIROOT: "\"$(mpifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""
   
   intel2022_impi2021:
      
      # Use GEOMAR libraries, not system modules
      iolibraries: geomar_libs
      
      add_module_actions:
         - "load slurm"
         - "load HLRNenv"
         - "load sw.skl"
         - "load cmake"
         - "load cdo nco"
         - "load git"
         - "load intel/2022.2"
         - "load impi/2021.6"
         - "load gcc/9.3.0" 
      add_export_vars:
         FC: mpiifort
         F77: mpiifort
         MPIFC: mpiifort
         FCFLAGS: -free
         CC: mpiicc
         CXX: mpiicpc
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""
   
   # For the new software stack on the new nodes      
   intel2023_impi2021:
      
      # Use system modules
      iolibraries: system_libs
      
      add_module_actions:
         - "load intel-oneapi-compilers/2023.2.1"
         - "load intel-oneapi-mpi/2021.10.0"
         # MKL needed for OpenIFS
         - "load intel-oneapi-mkl/2023.2.0" 
         # required for CMake
         #- "load curl/8.4.0-5rlmgmu ncurses/6.4-u72r7qn zlib-ng/2.1.4-ftbye2s"
         - "load cmake/3.27.7"
         # git conflicts
         #- "load git/2.42.0"
         
      
      # Note: Intel compilers now have new names: 
      # mpiicc (C) = mpiicx
      # mpiicpc (C++) = mpiicpx
      # mpiifort (Fortran) = mpiifx
      # 
      # OASIS compiles with mpiifx etc, but 
      # XIOS and OpenIFS do not, so we 
      # use the old mpiifort etc and live with the warnings
      add_export_vars:
         FC: mpiifort #mpiifx
         F77: mpiifort #mpiifx
         MPIFC: mpiifort #mpiifx
         FCFLAGS: -free
         CC: mpiicc #mpiicx
         CXX: mpiicpc #mpiicpx
         MPIROOT: "\"$(mpiifort -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
         MPI_LIB: "\"$(mpiifort -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

choose_iolibraries:

    system_libs:
        choose_compiler_mpi:
            intel2023_impi2021:
                # Modules to load 
                add_module_actions:
                    # Parallel HDF5
                    - "load hdf5/1.14.3"
                    - "load netcdf-c/4.9.2"
                    - "load netcdf-fortran/4.6.1-mpi"
                    # ecCodes required by OpenIFS
                    - "load eccodes/2.34.0"
                    # CDO loads eccodes/2.25.0 which conflicts with eccodes/2.34.0
                    #- "load cdo/2.2.2" 
                    - "load nco/5.1.6" 
                
                add_export_vars:
                    # Run module show on the modules to see how 
                    # each module changes or sets env variables
                    HDF5_ROOT: $HDF5_MODULE_INSTALL_PREFIX
                    HDF5ROOT: $HDF5_ROOT
                    NETCDF_DIR: $NETCDF_C_MODULE_INSTALL_PREFIX
                    NETCDFROOT: $NETCDF_C_MODULE_INSTALL_PREFIX
                    NETCDFFROOT: $NETCDF_FORTRAN_MODULE_INSTALL_PREFIX
                    ECCODESROOT: $ECCODES_MODULE_INSTALL_PREFIX 
                    LD_LIBRARY_PATH: $NETCDF_DIR/lib/:$LD_LIBRARY_PATH
                    NETCDF_CXX_LIBRARIES: $NETCDF_DIR/lib
                    
                    # For OASIS
                    HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
                    NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
                    NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
                    NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
                    OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'
                    
                    # For OASIS3-MCT5 from CERFACS
                    OASIS_NETCDF: $NETCDF_DIR
                    OASIS_NETCDFF: $NETCDF_DIR
         
    geomar_libs:
       add_export_vars:
          IO_LIB_ROOT: "<WILL_BE_OVERWERITTEN>"
          PATH: $IO_LIB_ROOT/bin:$PATH
          LD_LIBRARY_PATH: $IO_LIB_ROOT/lib:$LD_LIBRARY_PATH

          SZIPROOT: $IO_LIB_ROOT
          HDF5ROOT: $IO_LIB_ROOT
          HDF5_ROOT: $HDF5ROOT
          NETCDFROOT: $IO_LIB_ROOT
          NETCDFFROOT: $IO_LIB_ROOT
          ECCODESROOT: $IO_LIB_ROOT

          HDF5_C_INCLUDE_DIRECTORIES: $HDF5_ROOT/include
          NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
          NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
          OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'
          
          # For OASIS3-MCT5 from CERFACS
          OASIS_NETCDF: $IO_LIB_ROOT
          OASIS_NETCDFF: $IO_LIB_ROOT 

       choose_compiler_mpi:

          intel2019_impi2019_nemo4:
             add_export_vars:
                IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2019.0.5_impi2019.5_20200811

          intel2021_impi2021:
             add_export_vars:
                IO_LIB_ROOT: /home/shkjocke/sw/HPC_libraries/intel2021.2_impi2021.2_20211007
          
          # for now: use libraries compiled with older compilers
          intel2022_impi2021:
              add_export_vars:
                IO_LIB_ROOT:  /home/shkifmsw/sw/HPC_libraries/intel2022.2_impi2021.6_20230815 
                #IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2022.2_impi2021.6_20230719_save/ 
                
          intel2019_impi2019:
             add_export_vars:
                IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2019.0.5_impi2019.5_20200811

          intel2019_ompi:
             add_export_vars:
                IO_LIB_ROOT: /home/shkifmsw/sw/HPC_libraries/intel2019.0.5_ompi3.1.6_20201117

# some yamls use computer.fc, etc to identify the compiler, so we need to add them
fc: "$FC"
cc: "$CC"
mpifc: "$MPIFC"
mpicc: "$MPICC"
cxx: "$CXX"

launcher_flags: "--mpi=pmi2 -l --kill-on-bad-exit=1 --cpu_bind=${cpu_bind} --distribution=cyclic:cyclic --export=ALL"

further_reading:
        - batch_system/slurm.yaml
