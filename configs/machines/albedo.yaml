# The albedo.awi.de YAML config file
name: albedo
# Could be usefully extraced from /etc/os-distro or something
operating_system: "linux-rocky"
# Should not be defined here:
#jobtype: compute NOTE(PG): ...why?? The computer configuration should not need to know anything about the jobtype.
sh_interpreter: "/usr/bin/bash"


# Batch system and configuration:
batch_system: "slurm"
accounting: false
add_further_reading:
  - batch_system/slurm.yaml
partitions:
        compute:
                name: mpp
                cores_per_node: 36
        pp:
                name: smp
                cores_per_node: 1

# (compute) node hardware information
logical_cpus_per_core: 2
threads_per_core: 1


pool_directories:
        pool: "/dev/null"
        projects: "/dev/null"
        focipool: "/dev/null"
# PG: Why is this...? Can't we just reference a particular pool from the list each time?
pool_dir: "${computer.pool_directories.pool}"

hyper_flag: "" # No hyperthreading on albedo
choose_heterogeneous_parallelization:
        True:
                choose_taskset:
                    False:
                        add_export_vars:
                            I_MPI_SLURM_EXT: 0
                    "*":
                        taskset_chs: ""
                add_unset_vars:
                        - "SLURM_DISTRIBUTION"
                        - "SLURM_NTASKS"
                        - "SLURM_NPROCS"
                        - "SLURM_ARBITRARY_NODELIST"

hetjob_flag: packjob

unset_vars:
        - "SLURM_MEM_PER_NODE"
        - "SLURM_MEM_PER_CPU"

useMPI: intelmpi

module_actions:
        # Ensure a clean environment before start:
        - "purge"
        # Use my test modules (to be removed)
        - "use ~pgierz/esm_tools_albedo"
        # Compilers:
        - "load intel.compiler"
        # Build tools:
        - "load cmake"
        - "load automake"
        # Libraries:
        - "load udunits"
        - "load netcdf"
        - "load hdf5"
        # General Tools:
        - "load cdo"
        - "load nco"
        - "load git"
        # Show what is there in the log:
        - "list"

export_vars:
        # Basics:
        LC_ALL: en_US.UTF-8

        # Compilers:
        FC: ${fc}
        F77: ${f77}
        MPIFC: ${mpifc}
        MPICC: ${mpicc}
        CC: ${cc}
        CXX: ${cxx}

        # Message Passing Interface (MPI) environment variables:
        MPIROOT: "$(${mpifc} -show | perl -lne 'm{ -I(.*?)/include } and print $1')"
        MPI_LIB: "$(${mpifc} -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')"

        # HDF5 exported environment variables:
        HDF5ROOT: $HDF5_ROOT

        # NetCDF exported environment variables:
        NETCDFFROOT: $NETCDF_DIR
        NETCDFROOT: $NETCDF_DIR
        NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFROOT/include
        NETCDF_CXX_INCLUDE_DIRECTORIES: $NETCDFROOT/include
        NETCDF_CXX_LIBRARIES: $NETCDFROOT/lib

        # Linear-Algebra Library environment variables:
        LAPACK_LIB: '"-lmkl_intel_lp64 -lmkl_core -mkl=sequential -lpthread -lm -ldl"'
        LAPACK_LIB_DEFAULT: '"-L/global/AWIsoft/intel/2018/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential"'

choose_useMPI:
        intelmpi:
                add_module_actions:
                        - "unload intel.mpi"
                        - "load intel.mpi"
                fc: '"mpiifort -mkl"'
                f77: '"mpiifort -mkl"'
                mpifc: mpiifort
                mpicc: mpiicc
                cc: mpiicc
                cxx: mpiicpc

                add_export_vars:
                        MPIROOT: ${I_MPI_ROOT}/intel64
