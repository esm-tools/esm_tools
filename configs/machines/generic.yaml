# GENERIC YAML CONFIGURATION FILES
#name: bscearth383
name: generic
#hyper_flag: "--cpus-per-task=1"
additional_flags: "--mem=0"

accounting: true

"batch_system": "slurm"

jobtype: compute

choose_jobtype:
        post:
                partition: compute
        compute:
                partition: compute

choose_partition:
        "compute":
                cores_per_node: 24

logical_cpus_per_core: 2

threads_per_core: 1

pool_directories:
        pool: "/pool/data"


# standard environment setup
#
#

useMPI: intelmpi
module_actions:

export_vars:



choose_useMPI:
        intelmpi:
                add_module_actions:
                        - "unload intelmpi"
                        - "load intel/18.0.1 intelmpi/2018.1.163"
                        - "load autoconf/2.69"
                fc: mpiifort
                f77: mpiifort
                mpifc: mpiifort
                cc: mpiicc
                cxx: mpiicpc
        openmpi:
                add_module_actions:
                        - "load openmpi/2.0.2p2_hpcx-intel14"
                add_export_vars:
                        OMPI_MCA_pml: cm         # sets the point-to-point management layer
                        OMPI_MCA_mtl: mxm        # sets the matching transport layer
                        MXM_RDMA_PORTS: mlx5_0:1
                        MXM_LOG_LEVEL: ERROR
                        MXM_HANDLE_ERRORS: bt
                        UCX_HANDLE_ERRORS: bt
                fc: mpif90
                f77: mpif90
                mpifc: mpif90
                cc: mpicc
                cxx: mpicxx
        bullxmpi:
                add_module_actions:
                        - "load mxm/3.4.3082"
                        - "load fca/2.5.2431"
                        - "bullxmpi_mlx/bullxmpi_mlx-1.2.9.2"
                add_export_vars:
                        FC: mpif90
                        F77: mpif90
                        MPIFC: mpif90
                        CC: mpicc
                        CXX: mpicxx

