################################################################################
# The albedo.awi.de yaml config file for esm-tools
#
# P. Gierz, Nov/Dec 2022
# M. Andres-Martinez, Nov/Dec 2022
################################################################################
name: albedo
# Could be useful if this is extraced from /etc/os-distro:
operating_system: "linux-rocky"
sh_interpreter: "/usr/bin/bash"
# Batch system and configuration:
batch_system: "slurm"
accounting: true
further_reading:
  - batch_system/slurm.yaml
partitions:
  compute:
    name: mpp
    cores_per_node: 36
  pp:
    name: smp
    cores_per_node: 1

# compute node hardware information:
logical_cpus_per_core: 2
threads_per_core: 1

# pool directory information:
pool_directories:
  pool: "/albedo/pool"
  projects: "/albedo/work/projects"
  focipool: "/dev/null"
pool_dir: "${computer.pool_directories.pool}"

hyper_flag: "" # No hyperthreading on albedo is supported in esm-tools (yet)!
choose_heterogeneous_parallelization:
  True:
    choose_taskset:
      False:
          add_export_vars:
              I_MPI_SLURM_EXT: 0
      "*":
          taskset_chs: ""
    add_unset_vars:
      - "SLURM_DISTRIBUTION"
      - "SLURM_NTASKS"
      - "SLURM_NPROCS"
      - "SLURM_ARBITRARY_NODELIST"

hetjob_flag: packjob

unset_vars:
  - "SLURM_MEM_PER_NODE"
  - "SLURM_MEM_PER_CPU"

mpi_implementation: openmpi
choose_mpi_implementation:
  intel-oneapi-mpi:
    mpi_implementation_module: intel-oneapi-mpi/2021.6.0
    mpi_implementation_suffix: intel-oneapi-mpi2021.6.0
  openmpi:
    mpi_implementation_module: openmpi/4.1.3
    mpi_implementation_suffix: openmpi4.1.3

compiler_suite: gcc
choose_compiler_suite:
  gcc:
    compiler_module: gcc/12.1.0
    compiler_suffix: gcc12.1.0
  intel-oneapi:
    compiler_module: intel-oneapi-compilers/2022.1.0
    compiler_suffix: oneapi2022.1.0

netcdf_c_version: "4.8.1"
netcdf_cxx_version: "4.2"
netcdf_fortran_version: "4.5.4"
hdf5_version: "1.12.2"

module_actions:
        # Ensure a clean environment before start:
        - "purge"
        # Compilers:
        - "load ${compiler_module}"
        # MPI Implementation:
        - "load ${mpi_implementation_module}"
        # Build Tools:
        - "load /albedo/soft/modules/spack-modules/cmake/3.23.1-gcc12.1.0-o42z"
        # Libraries:
        # FIXME(PG): @sebhinck do we need to get a specific udunits version here as well, maybe?
        - "load udunits/2.2.28"
        # NOTE(PG): The module versions **with** mpi in the module name are for parallel NetCDF, not needed in many cases
        - "load netcdf-c/${netcdf_c_version}-${compiler_suffix}"
        - "load netcdf-cxx/${netcdf_cxx_version}-${compiler_suffix}"
        - "load netcdf-fortran/${netcdf_fortran_version}-${compiler_suffix}"
        - "load hdf5/${hdf5_version}-${compiler_suffix}"
        # - "load intel-oneapi-mkl/2022.1.0"  # Math Kernel Library
        - "load /albedo/soft/modules/spack-modules/intel-oneapi-mkl/2022.1.0-gcc12.1.0-akth"
        # General Tools:
        - "load cdo/2.0.5"
        - "load nco/5.0.1"
        - "load git/2.35.2"
        - "load python/3.10.4"
        # Show what is there in the log:
        - "list"

export_vars:
  # Basics:
  LC_ALL: en_US.UTF-8

  # Compilers:
  # FC: ${fc}
  # F77: ${f77}
  # MPIFC: ${mpifc}
  # MPICC: ${mpicc}
  # CC: ${cc}
  # CXX: ${cxx}

  # Message Passing Interface (MPI) environment variables:
  # MPIROOT: "$(${mpifc} -show | perl -lne 'm{ -I(.*?)/include } and print $1')"
  MPI_LIB: "$($MPIF90 -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')"

  # Linear-Algebra Library environment variables:
  # FIXME(PG): This would be nicer...
  # LAPACK_LIB: "$(mkl_link_tool --quiet -libs $MPIF90)"
  LAPACK_LIB: "'-L/albedo/soft/sw/spack-sw/intel-oneapi-mkl/2022.1.0-7235vfh/mkl/2022.1.0 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lm -ldl'" 
