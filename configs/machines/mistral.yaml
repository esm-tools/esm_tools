# MISTRAL YAML CONFIGURATION FILES

name: mistral
#hyper_flag: "--cpus-per-task=1"
additional_flags: ["--mem=0"]
account: None

use_hyperthreading: False
choose_use_hyperthreading:
        "1":
                hyperthreading_flag: ""
        True:
                hyperthreading_flag: ""
        "0":
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                choose_taskset:
                                    False:
                                        add_export_vars:
                                            I_MPI_SLURM_EXT: 0
                                    "*":
                                        taskset_chs: ""
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"
        False:
                choose_heterogeneous_parallelization:
                        False:
                                hyperthreading_flag: "--ntasks-per-core=1"
                        True:
                                hyperthreading_flag: ""
                                choose_taskset:
                                    False:
                                        add_export_vars:
                                            I_MPI_SLURM_EXT: 0
                                    "*":
                                        taskset_chs: ""
                                add_unset_vars:
                                        - "SLURM_DISTRIBUTION"
                                        - "SLURM_NTASKS"
                                        - "SLURM_NPROCS"
                                        - "SLURM_ARBITRARY_NODELIST"

accounting: true
hetjob_flag: packjob

"batch_system": "slurm"

jobtype: compute
sh_interpreter: "/bin/bash"


partition: compute

choose_partition:
        "compute":
                partition_name: "compute"
                partition_cpn: 24
        "compute2":
                partition_name: "compute2"
                partition_cpn: 36
        "compute,compute2":
                partition_name: "compute,compute2"
                partition_cpn: 72

partitions:
        compute:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}
        pp:
                name: ${computer.partition_name}
                cores_per_node: ${computer.partition_cpn}


logical_cpus_per_core: 2

threads_per_core: 1

pool_directories:
        pool: "/pool/data"
        focipool: "/work/bb0519/foci_input2"

pool_dir:  "/pool/data"
# standard environment setup
#
#

useMPI: intelmpi
module_actions:
        - "purge"
        - "unload netcdf_c"
        - "unload intel intelmpi"
        - "load python3/2021.01-gcc-9.1.0"
        - "load cmake/3.13.3"
        - "load autoconf/2.69"
        - "load nco"
        - "load cdo"
        - "load gcc/4.8.2"

export_vars:
        #########################################################################
        # Locale Settings
        #########################################################################
        #
        LC_ALL: en_US.UTF-8

        #########################################################################
        # Compiler Settings
        #########################################################################
        #
        FC: ${fc}
        F77: ${f77}
        MPIFC: ${mpifc}
        CC: ${cc}
        CXX: ${cxx}

        #########################################################################
        # MPI Settings
        #########################################################################
        #
        MPIROOT: "\"$(${mpifc} -show | perl -lne 'm{ -I(.*?)/include } and print $1')\""
        MPI_LIB: "\"$(${mpifc} -show |sed -e 's/^[^ ]*//' -e 's/-[I][^ ]*//g')\""

        #########################################################################
        # HDF5 Defaults
        #########################################################################
        #
        # Why are we not using MPI HDF5?
        #- 'HDF5ROOT=/sw/rhel6-x64/hdf5/hdf5-1.8.16-parallel-impi-intel14/'
        #- 'HDF5_C_INCLUDE_DIRECTORIES=$HDF5ROOT/include'
        # NOTE(JK): Using wrong combination of HDF5, netCDF-C and netCDF-F libs can lead to
        #           undefined symbol: __intel_skx_avx512_memcpy
        #           This combination was recommended by DKRZ support
        HDF5ROOT: /sw/rhel6-x64/hdf5/hdf5-1.8.14-parallel-impi-intel14/
        HDF5_C_INCLUDE_DIRECTORIES: $HDF5ROOT/include
        # NOTE(PG): The version of HDF5_ROOT **WITH** an underscore is used by PISM Cmake.
        #           Moved to PISM yamls, but doesn't show up??
        HDF5_ROOT: $HDF5ROOT

        #########################################################################
        # NETCDF Defaults
        #########################################################################
        #
        # NOTE(PG): Is it intentional to have different netcdf_fortran and
        # netcdf_c versions?
        NETCDFFROOT: /sw/rhel6-x64/netcdf/netcdf_fortran-4.4.2-parallel-impi-intel14/
        NETCDFROOT: /sw/rhel6-x64/netcdf/netcdf_c-4.3.2-parallel-impi-intel14/
        # NOTE(PG): The next lines are moved to echam.yaml, and only active for:
        # A) mistral
        # and
        # B) echam-6.3.05p2-concurrent-radiation-paleodyn
        # the next line is the critical one, this works:
        # - "NETCDFROOT=/sw/rhel6-x64/netcdf/netcdf_c-4.3.2-gcc48"
        # this doesn't - mismatch with cxx_include_directories???:
        # - "NETCDFROOT=/sw/rhel6-x64/netcdf/netcdf_c-4.3.2-parallel-impi-intel14/"
        NETCDF_Fortran_INCLUDE_DIRECTORIES: $NETCDFFROOT/include
        NETCDF_C_INCLUDE_DIRECTORIES: $NETCDFROOT/include
        NETCDF_CXX_INCLUDE_DIRECTORIES: /sw/rhel6-x64/netcdf/netcdf_cxx-4.2.1-gcc48/include

        #########################################################################
        # Linear Algebra (LAPACK)
        #########################################################################
        #
        LAPACK_LIB: "'-mkl=sequential'"
        LAPACK_LIB_DEFAULT: "'-L/sw/rhel6-x64/intel/intel-18.0.1/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential'"

        #########################################################################
        # General Software (Perl, usw.)
        #########################################################################
        #
        PERL5LIB: /usr/lib64/perl5
        SZIPROOT: /sw/rhel6-x64/sys/libaec-0.3.2-gcc48
        ZLIBROOT: /usr

        #########################################################################
        # seb-wahl: OASIS3MCT_FC_LIB is used by ECHAM, since it's unclear whether
        # it's used by other models it stays in mistral.yaml for now
        #########################################################################
        OASIS3MCT_FC_LIB: '"-L$NETCDFFROOT/lib -lnetcdff"'

        #########################################################################
        # LD_LIBRARY_PATH
        #
        # Since the LD_LIBRARY_PATH modifies linking, this block probably needs
        # to be at the end!!
        #########################################################################
        #
        LD_LIBRARY_PATH[(0)]: $NETCDFROOT/lib:$NETCDFFROOT/lib:$HDF5ROOT/lib:$LD_LIBRARY_PATH
        # avoid GLIBCXX_3.4.15 not found error
        LD_LIBRARY_PATH[(1)]: /sw/rhel6-x64/gcc/gcc-4.8.2/lib64:$LD_LIBRARY_PATH
        # The following line is also moved to echam.yaml, only active for mistral and paleodyn echam6:
        # - "LD_LIBRARY_PATH=/sw/rhel6-x64/netcdf/parallel_netcdf-1.6.1-impi-intel14/lib/:$LD_LIBRARY_PATH"

        ################################################
        # This needs to be moved into OIFS
        # MPI environent variables important at runtime
        I_MPI_FABRICS: shm:dapl
        I_MPI_FALLBACK: disable
        I_MPI_SLURM_EXT: 1
        I_MPI_LARGE_SCALE_THRESHOLD: 8192
        I_MPI_DYNAMIC_CONNECTION: 1

        DAPL_NETWORK_NODES: $SLURM_NNODES
        DAPL_NETWORK_PPN: $SLURM_NTASKS_PER_NODE
        DAPL_WR_MAX: 500

        OMPI_MCA_pml: cm
        OMPI_MCA_mtl: mxm
        OMPI_MCA_coll: ^ghc
        OMPI_MCA_mtl_mxm_np: 0

        MXM_RDMA_PORTS: mlx5_0:1
        MXM_LOG_LEVEL: FATAL
        ################################################

choose_useMPI:
        intelmpi:
                add_module_actions:
                        - "unload intel intelmpi"
                        - "load intel/18.0.4 intelmpi/2018.5.288"
                        - "load libtool/2.4.6"
                        - "load automake/1.14.1"
                fc: mpiifort
                f77: mpiifort
                mpifc: mpiifort
                cc: mpiicc
                cxx: mpiicpc


        intel18_bullxmpi:
                add_module_actions:
                        - "unload intel intelmpi"
                        - "load intel/18.0.4"
                        - "load libtool/2.4.6"
                        - "load autoconf/2.69"
                        - "load automake/1.14.1"
                        - "load bullxmpi_mlx/bullxmpi_mlx-1.2.9.2"
                fc: mpif90
                f77: mpif90
                mpifc: mpif90
                cc: mpicc
                cxx: mpicxx

        intelmpi17:
                add_module_actions:
                        - "unload intel intelmpi"
                        - "load intel/17.0.6 intelmpi/2017.3.196"
                fc: mpiifort
                f77: mpiifort
                mpifc: mpiifort
                cc: mpiicc
                cxx: mpiicpc
                add_export_vars:
                        LAPACK_LIB_DEFAULT: '-L/sw/rhel6-x64/intel/intel-17.0.6/mkl/lib/intel64 -lmkl_intel_lp64 -lmkl_core -lmkl_sequential'
        openmpi:
                add_module_actions:
                        - "load intel/18.0.4"
                        - "unload cmake"
                        - "load cmake"
                        - "load openmpi/2.0.2p2_hpcx-intel14"
                        - "unload netcdf"
                        - "load netcdf_c/4.3.2-gcc48"
                add_export_vars:
                        I_MPI_SLURM_EXT: 0

                        MXM_LOG_LEVEL: ERROR
                        I_MPI_CHECK_DAPL_PROVIDER_COMPATIBILITY: 0
                        I_MPI_HARD_FINALIZE: 1
                        I_MPI_ADJUST_GATHER: 1 # do not use =3 (Shumilin's algorithm)

                        HDF5ROOT: "/sw/rhel6-x64/hdf5/hdf5-1.8.14-threadsafe-gcc48"
                        HDF5_C_INCLUDE_DIRECTORIES: "/sw/rhel6-x64/hdf5/hdf5-1.8.14-threadsafe-intel14/include"
                        NETCDFFROOT: "/sw/rhel6-x64/netcdf/netcdf_fortran-4.4.3-intel14"
                        NETCDFROOT: "/sw/rhel6-x64/netcdf/netcdf_c-4.4.0-gcc48"

                fc: mpif90
                f77: mpif90
                mpifc: mpif90
                cc: mpicc
                cxx: mpicxx
                remove_computer.export_vars: [MPIROOT, MPI_LIB, DAPL_NETWORK_NODES, DAPL_NETWORK_PPN, DAPL_WR_MAX, OMPI_MCA_pml, OMPI_MCA_mtl, OMPI_MCA_coll, OMPI_MCA_mtl_mxm_np, MXM_RDMA_PORTS]

        bullxmpi:
                add_module_actions:
                        - "load mxm/3.4.3082"
                        - "load fca/2.5.2431"
                        - "bullxmpi_mlx/bullxmpi_mlx-1.2.9.2"
                add_export_vars:
                        FC: mpif90
                        F77: mpif90
                        MPIFC: mpif90
                        CC: mpicc
                        CXX: mpicxx


further_reading:
        - batch_system/slurm.yaml
